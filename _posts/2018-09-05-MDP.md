---
title:  “马尔科夫决策过程”
date:   2018-09-05 08:00:12 +0800
categories: machine learning
---

Ng Andrew的斯坦福机器学习课程CS229从第16讲开始进入强化学习(Reinforcement Learning)。马尔科夫决策过程具有马尔科夫性，即系统下一个状态只和当前状态和当前采取的动作相关，而与更早之前状态无关。

几个马尔科夫相关概率如下图：

 | 不考虑动作 |  考虑动作
:----|:----|:----
状态完全可见 | 马尔科夫链（Markov Chain） | 马尔科夫决策过程（Markov Decision Process）
状态不完全可见 | 隐马尔科夫模型（Hidden Markov Model） | 部分可观察马尔科夫决策过程（Partially Observable Markov Decision Process)


马尔科夫决策过程由一个五元组$$(S, A, \{P_{sa}\}, \gamma, R)$$构成，其中：
- $$S$$表示状态集合（States）
- $$A$$表示一组动作（Actions）
- $$P_{sa}$$是状态转移概率分布，表示在当前$$s \in S$$状态下，经过$$a \in A$$作用后，转移到的其他状态的概率，$$\sum_{s'} P_{sa}(s') = 1, P_{sa}(s') \le 0$$
- $$\gamma$$表示贴现因子（discount factor）
- $$R$$表示奖励函数

马尔科夫决策过程可以表示为下图：

![image01]({{site.baseurl}}/image/20180905/mdp.jpg)

这个过程的奖励函数和为：

$$
R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \cdots
$$

目标应该是使得奖励函数和均值最大：

$$
max E[R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \cdots]
$$

我们需要知道在当前状态下应该采取什么样的行动，动作的选择过程称为策略（Policy），定义为$$S$$到$$A$$的映射函数$$\pi: S \mapsto A$$。对于策略$$\pi$$，定义值函数（Value Function）$$V^\pi: S \mapsto \mathbb{R} $$：

$$
\begin{aligned}
V^\pi(s) &= E[R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \cdots | s_0=s, \pi] \\
&= R(s_0) + \gamma(E[R(s_1) + \gamma R(s_2) + \cdots | s_0=s, \pi) \\
&= R(s) + \gamma \sum_{s' \in S} P_{s\pi(s)}(s')V^\pi(s')
\end{aligned}
$$

上式称为贝尔曼方程（Bellman Equation），第一部分$$R(s)$$表示即刻回报（Immediate Reward），第二部分$$\gamma \sum_{s' \in S} P_{s\pi(s)}(s')V^\pi(s')$$表示未来贴现回报（Future Discounted Reward）。假设$$S$$集合中有n个状态，则有个n个变量$$V^\pi(s)$$，n个贝尔曼等式。通过求解贝尔曼方程组，即可获得$$V^\pi(s)$$。

然后我们的目标是找到使得$$V^\pi(s)$$最大的$$\pi$$:

$$
V^*(s) = \max_\pi V^\pi(s)
$$

对应贝尔曼方程的表达形式为：

$$
V^*(s) = R(s) + \max_{a \in A} \gamma \sum_{s' \in S} P_{sa}(s')V^*(s')
$$

我们定义最佳策略$$\pi^*: S \mapsto A$$：

$$
\pi^*(s) = \arg \max_{a \in A} \sum_{s' \in S} P_{sa}(s')V^*(s')
$$

求解此最优化问题有两个算法

- 价值迭代（Value Iteration）

    ![image02]({{site.baseurl}}/image/20180905/value_iteration.png)

- 策略迭代（Policy Iteration）

    ![image03]({{site.baseurl}}/image/20180905/policy_iteration.png)

## 连续状态MDP

上述讨论的马尔科夫决策过程中的状态都是离散的，比如推箱子例子中的位置信息$$(x, y)$$。而某些情况下状态可能是连续值。比如Inverted Pendulum问题中，某一时刻的状态为$$(x, \beta, \dot{x}, \dot{\beta})$$，分别表示位置，角度，速度和角速度。

一个解决方案是将连续状态值离散化，如上述例子中将每个维度分成$$k$$个区间。但是这样可能会造成维度灾难，对于$$n$$维状态，状态集合$$S$$将有$$k^n$$个离散值。

另一个算法是Fitted Value Iteration。首先需要假设一个Simulator，输入一个状态$$s_t$$和一个操作$$a_t$$（或者用$$s,a$$表示），可以得到下一个状态$$s_{t+1}$$（或者用$$s'$$表示），且服从概率$$P_{sa}$$。

![image04]({{site.baseurl}}/image/20180905/simulator.jpg)

Simulator可以根据物理公式推导出一个确定性模型；或者假设一个线性/非线性模型，再根据监督学习算法拟合参数。

线性假设模型如下：

$$
s_{t+1} = As_t + Ba_t + \epsilon_t
$$

非线性假设模型如下：

$$
s_{t+1} = A\phi_s(s_t) + B\phi_a(a_t)
$$

在价值迭代算法中，使用期望定义转换：

$$
\begin{aligned}
V(s) &= R(s) + \gamma \max_{a \in A} \int_{s'} P_{sa}(s')V(s')ds' \\
&= R(s) + \gamma \max_{a \in A} E_{s' \sim P_{sa}} [V(s')]
\end{aligned}
$$

令$$V(s) = \theta^T \phi(s)$$进行价值拟合，我们需要估计系数$$\theta$$的值。$$\phi(s)$$可以用位置和速度的平方或乘积作为特征：

$$
\phi(s) = s \quad or \quad \phi(s) = \left [
        \begin{matrix}
            1 \\
            x \\
            \dot{x} \\
            \dot{x}^2 \\
            \beta x \\
            \dot{\beta}^2
        \end{matrix}
    \right ]
$$

在拟合价值迭代算法中，通过$$P_{sa}$$分布下的采样，来估计$$R(s) + \gamma \max_{a \in A} E_{s' \sim P_{sa}} [V(s')]$$的取值。

![image05]({{site.baseurl}}/image/20180905/fitted_value_iteration.jpg)

如果Simulator模型是确定的$$s_{t+1} = f(s_t, a_t) + \epsilon$$，则

$$
\begin{aligned}
E_{s' \sim P_{sa}} [V^*(s')] &\approx V^*(E[s']) \\
&= V^*(f(s, a))
\end{aligned}
$$

$$V^*(f(s, a))$$用$$\theta^T \phi(s)$$来拟合，其中$$\phi(s) = f(s,a)$$（这个地方还是没看太懂）

参考：  
[csdn - Andrew Ng机器学习课程笔记之强化学习](https://blog.csdn.net/danerer/article/details/80417136)  
[csdn - cs229学习笔记6(MDP)](https://blog.csdn.net/Dark_Scope/article/details/8252969)