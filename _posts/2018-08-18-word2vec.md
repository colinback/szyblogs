---
title:  “Spark ML - Word2Vec”
date:   2018-08-18 08:00:12 +0800
categories: machine learning
---

Word2Vec是Google 2013年开源推出的一个获取词向量的工具包。没办法，大牛公司就是搞什么大家就跟风什么。很意外的工作里居然涉及到了一点。

网上搜了一下原理，基本上只需要看这哥们的文章[Word2Vec的数学原理](https://blog.csdn.net/itplus/article/details/37969519)就可以了。为了方便查阅，在自己的博客整理了一个简版。


# **词向量**

词典$$ \mathcal D $$中的任意词$$ w $$，对应一个固定长度的实值向量$$ v(w) \in \mathbb R^m $$，就称为词向量。
- 一种最简单的词向量就是`one-hot representation`，用一个很长的向量来表示一个词$$ w $$，向量长度为词典$$ \mathcal D $$的大小N，分量1的位置对应该词在词典中的索引，其他分量全为0。很显然这会带来维度灾难。
- 另一种词向量是`Distributed representation`，通过训练将某种语言的每一个词映射成一个固定长度的“短”向量。Word2Vec采用的即是此种词向量。

    有很多模型可以用来估计词向量，比如LSA（Latent Semantic Analysis）和LDA（Latent Dirichlet Allocation）。此外神经网络算法也是一种常用方法，而且大部分情况下，词向量和语言模型捆绑在一起，训练完成后两者同时得到。

    词向量可以分析词之间的相关，网上一个例子就是king-Queen-Man-Women。假设这四个词背后隐含的四个维度分别是‘Royalty','Masculinity', 'Femininity', 'Age'：

    | | King | Queen | Man | Woman
    :---|:---|:---|:---|:---
    Royalty | 0.99 | 0.99 | 0.03 | 0.02
    Masculinity | 0.99 | 0.05 | 0.95 | 0.01
    Femininity | 0.05 | 0.93 | 0.06 | 0.999
    Age | 0.7 | 0.6 | 0.5 | 0.5

    我们可以发现：
    $$
    \hat{Queen} = \hat{King} - \hat{Man} + \hat{Woman}
    $$

    ![image01]({{site.baseurl}}/image/20180818/word_vector.png)
    
按粒度进行推广，可以得到字向量，句向量或者文档向量。

# **统计语言模型**

吴军的《数学之美》里提到过这个，在多年语义/词义分析方法处理自然语言无法突破之后，NLP转向了统计方法。统计语言模型是计算一个句子概率的概率模型，比如一个句子W由T个单词$$ w_1,w_2,\cdots,w_T $$按顺序构成，其联合概率为：

$$
\begin{aligned}
p(W) &= p(w_1, w_2, \cdots, w_t) \\
&= p(w_1) p(w_2|w_1) p(w_3|w_1, w_2) \cdots p(w_T|w_1, w_2, \cdots, w_{T-1})
\end{aligned}
$$

上式利用了贝叶斯公式，然而计算所有的
$$ p(w_k|w_1, w_2, \cdots, w_{k-1}) $$概率是很恐怖的，下面看看怎么办。

## n-gram模型

首先想到的就是近似，采用n-1阶的Markov链假设，即一个词出现的概率只和它前面的n-1个单词相关。根据大数定律，当语料库足够大时，统计词串出现的次数来表示概率。

$$
\begin{aligned}
p(w_k|w_{k-n+1}, \cdots, w_{k-1}) &= \frac{p(w_{k-n+1}, \cdots, w_k)}{p(w_{k-n+1}, \cdots, w_{k-1})} \\
&\approx \frac{count(w_{k-n+1}, \cdots, w_k)}{count(w_{k-n+1}, \cdots, w_{k-1})}
\end{aligned}
$$

n=1时各个单词出现的概率是独立的。n不能取得太大，实际应用中最多的是采用n=3的三元模型。

## 神经概率语言模型

**机器学习领域的通用招数是这样的：对所考虑问题建模，为其构造一个目标函数，然后对这个目标函数优化，求得一组参数，最后通过这组参数对应的模型进行预测。**

对于统计模型，目标参数是最大对数似然函数：

$$
\mathcal{L} = \sum_{w \in \mathcal{C}} \log p(w|Context(w))
$$

其中
$$ \mathcal{C} $$表示语料，$$ Context(w) $$表示词的上下文，即周边词的集合。我们将$$ p(w|Context(w)) $$视为$$ w $$和$$ Context(w) $$的函数，$$ \theta $$为待定参数：

$$
p(w|Context(w)) = F(w, Context(w); \theta)
$$

一旦构造出F函数，通过最大对数似然函数导数得到最优参数集$$ \theta^* $$，问题即可得解。下面就是2003年提出的通过神经网络构造F的方法：

![image02]({{site.baseurl}}/image/20180818/nn_nlp.png)

对于语料$$ \mathcal{C} $$中的任意一个词$$ w $$，将$$ Context(w) $$取其前面的n-1个词，于是二元对$$ (Context(w),w) $$成为一个训练样本。词向量长度为m，
$$ N = |\mathcal{D}| $$是语料$$ \mathcal{C} $$的词汇量大小，即字典大小。将输入层的n-1个词向量按顺序首尾相接，组成一个长向量作为投影层向量$$ x_w $$

$$
\begin{aligned}
&z_w = tanh(Wx_w + p) \\
&y_w = Uz_w + q
\end{aligned}
$$

tanh为双曲正切函数，用来做隐藏层的激活函数。$$ y_w $$是一个N维向量，如果想要其分量表示当上下文为$$ Context(w) $$时的下一个词是词典中第i个词的概率，需要做一个softmax归一化:

$$
p(w|Context(w)) = \frac{e^{y_{wi_w}}}{\sum_{i=1}^N e^{y_{wi}}}
$$

待确定的参数包括：
- 词向量：$$ v(w) \in \mathbb R^m, w \in \mathcal{D} $$
- 神经网络参数：$$ W \in \mathbb R^{n_h \times (n-1)m}, p \in \mathbb R^{n_h}, U \in \mathbb R^{N \times n_h}, q \in \mathbb R^N $$

神经概率语言模型是Word2Vec算法的前身。

# **Word2Vec**

这才开始进入正题，Word2Vec有两个模型：CBOW(Continuous Bag-of-Words Model)和Skip-gram(Continuous Skip-gram Model)。CBOW模型根据上下文的n-1个词来预测出这个词本身；Skip-gram模型根据词本身来预测周围有哪些词。

![image03]({{site.baseurl}}/image/20180818/word2vec_model.png)

谷歌给两个模型设计了两套框架，分别基于Hierarchical Softmax和Negative Sampling。

## Hierarchical Softmax

1. CBOW模型

    CBOW模型包含三层
    - 输入层：$$ Context(w) $$中2c个词的词向量$$ v(Context(w)_i) \in \mathbb R^m $$
    - 投影层：将输入层2c个向量做求和累加，$$ x_w = \sum_{i=1}^{2c} v(Context(w)_i) $$
    - 输出层：一棵二叉树，以各词在语料中出现次数当做权值构造出来的Huffman树，叶子节点N个，非叶子节点N-1个

    ![image04]({{site.baseurl}}/image/20180818/cbow.png)

    先定义几个符号：
    1. $$ p^w $$: 从根节点出发到$$ w $$对应的叶子节点路径
    2. $$ l^w $$: 路径$$ p^w $$包含节点个数
    3. $$ p_1^w, p_2^w, \cdots, p_{l^w}^w $$：路径$$ p^w $$中的$$ l^w $$个节点，其实$$ p_1^w $$表示根节点，$$ p_{l^w}^w $$表示词$$ w $$对应的节点。
    4. $$ d_2^w, d_3^w, \cdots, d_{l^w}^w \in {0, 1} $$: 词$$ w $$的Huffman编码，由$$ l^w-1 $$位编码构成，根节点不对应编码。
    5. $$ \theta_1^w, \theta_2^w, \cdots, \theta_{l^w-1}^w \in \mathbb R^m $$：路径$$ p^w $$中非叶子节点对应的向量（隐藏参数）

    作者很贴心的给了一个例子说明：

    ![image05]({{site.baseurl}}/image/20180818/football.png)

    红色边连起来的5个节点构成一个预测节点为“足球”的路径$$ p^w $$，其长度$$ l^w = 5 $$。$$ p_1^w, p_2^w, p_3^w, p_4^w, p_5^w $$为路径上5个节点，$$ p_1^w $$对应根节点，$$ d_2^w, d_3^w, d_4^w, d_5^w $$分别为1, 0, 0, 1，即“足球”Huffman编码为1001。

    下面的推导很有意思。首要目标是对
    $$ p(w|Context(w)) $$建模，如何将预测概率和Huffman树联系起来？方法是树的每一次分支看做二分类。规定将一个节点进行分类时，分到左边是负类（编码1），分到右边是正类（编码0）。于是根据逻辑回归，一个节点被正确分为正类的概率是

    $$
    \sigma(x_w^T\theta) = \frac{1}{1 + e^{-x_w^T\theta}}
    $$

    于是目标条件概率可以写成：

    $$
    \begin{aligned}
    p(w|Context(w)) &= \prod_{j=2}^{l^w} p(d_j^w|x_w; \theta_{j-1}^w) \\
    &=  \prod_{j=2}^{l^w} [\sigma(x_w^T\theta_{j-1}^w)]^{1 - d_j^w} \centerdot [1 - \sigma(x_w^T\theta_{j-1}^w)]^{d_j^w} 
    \end{aligned}
    $$

    下面开始满满的都是套路。目标函数代入上式，要使得对数似然函数最大。

    $$
    \mathcal{L} = \sum_{w \in \mathcal{C}} \sum_{j=2}^{l^w} \left( (1-d_j^w) \log(\sigma(x_w^T\theta_{j-1}^w)) + d_j^w \log(1 - \sigma(x_w^T\theta_{j-1}^w)) \right )
    $$

    采用了随机梯度上升法，每次取一个样本$$ (Context(w), w) $$对目标函数中的参数更新，参数包括$$ x_w, \theta_{j-1}^w, w \in \mathcal{C}, j = 2, \cdots, l^w $$。对双重求和中的部分定义为$$ L(w,j) $$，求偏导：

    $$
    \begin{aligned}
    \frac{\partial L(w,j)}{\partial \theta_{j-1}^w} &= \frac{\partial}{\partial \theta_{j-1}^w} \left( (1-d_j^w) \log(\sigma(x_w^T\theta_{j-1}^w)) + d_j^w \log(1 - \sigma(x_w^T\theta_{j-1}^w)) \right ) \\
    &= (1 - d_j^w)(1 - \sigma(x_w^T\theta_{j-1}^w))x_w - d_j^w\sigma(x_w^T\theta_{j-1}^w)x_w \\
    &= [1 - d_j^w - \sigma(x_w^T\theta_{j-1}^w)]x_w
    \end{aligned}
    $$

    于是$$ \theta_{j-1}^w $$更新公式为：

    $$
    \theta_{j-1}^w := \theta_{j-1}^w + \eta[1 - d_j^w - \sigma(x_w^T\theta_{j-1}^w)]x_w
    $$

    其中$$ \eta $$表示学习率。然后是$$ x_w $$的更新，其表示的是$$Context(w) $$中各词词向量的累加。

    $$
    \begin{aligned}
    & \frac{\partial L(w,j)}{\partial x_w} = [1 - d_j^w - \sigma(x_w^T\theta_{j-1}^w)]\theta_{j-1}^w \\
    & v(u) := v(u) + \eta \sum_{j=2}^{l^w} \frac{\partial L(w,j)}{\partial x_w}, u \in Context(w)
    \end{aligned}
    $$

    上式的意义是把梯度的$$ x_w $$分量$$ \sum_{j=2}^{l^w} \frac{\partial L(w,j)}{\partial x_w} $$贡献到$$ Context(w) $$的每一个词的词向量上。

    伪代码如下：

    ![image06]({{site.baseurl}}/image/20180818/cbow_pseudo.png)

2. Skip-gram模型

    Skip-gram模型也分输入层，投影层和输出层。输入是当前样本的中心词$$ w $$的词向量$$ v(w) \in \mathbb{R^m} $$；投影层是恒等投影，有点多余；输出层同样是一棵Huffman树。

    ![image07]({{site.baseurl}}/image/20180818/skip_gram.png)

    几乎类似的推导过程可得：

    $$
    \begin{aligned}
    p(Context(w)|w) &= \prod_{u \in Context(w)} p(u|w) \\
    &= \prod_{u \in Context(w)} \prod_{j=2}^{l^u} p(d_j^u|v(w); \theta_{j-1}^u)
    \end{aligned}
    $$

    $$
    \mathcal{L} = \sum_{w \in \mathcal{C}} \sum_{u \in Context(w)} \sum_{j=2}^{l^u} \left( (1-d_j^u) \log(\sigma(v(w)^T\theta_{j-1}^u)) + d_j^u \log(1 - \sigma(v(w)^T\theta_{j-1}^u)) \right )
    $$

    更新公式：

    $$
    \begin{aligned}
    & \theta_{j-1}^u := \theta_{j-1}^u + \eta[1 - d_j^u - \sigma(v(w)^T\theta_{j-1}^u)]v(w) \\
    & \frac{\partial L(w,u,j)}{\partial v(w)} = [1 - d_j^u - \sigma(v(w)^T\theta_{j-1}^u)]\theta_{j-1}^u \\
    & v(w) := v(w) + \eta \sum_{u \in Context(w)} \sum_{j=2}^{l^u} \frac{\partial L(w,u,j)}{\partial x_w}
    \end{aligned}
    $$

    有个小处理是，并不是等$$ Context(w) $$中所有词处理完后才更新$$ v(w) $$，而是每次处理完$$ Context(w) $$中的一个词$$ u $$，就及时更新一次$$ v(w) $$。伪代码如下：

    ![image08]({{site.baseurl}}/image/20180818/skip_gram_pseudo.png)

## Negative Sampling

负采样算法，目的是提高训练速度和改善词向量质量。大致的意思是随机在$$ (Context(w), u) $$采样，选出一个负样本子集（$$ u \neq w $$），和一个正样本一起作为样本空间。最大化目标函数是：

$$
g(w) = \prod_{u \in \{w\} \cup NEG(w)} p(u|Context(w))
$$

后面的推导不再罗列。Negative Sampling说是NCE(Noise Contrastive Estimation)的简化，我没有太理解其原理。有兴趣直接看后面`Word2Vec的数学原理`的链接吧。

# **Spark ML 2.3.1**

Spark ML中的Word2Vec采用了基于Hierarchical Softmax的skip-gram模型。模型默认词向量维度为100，窗口大小5，即$$ Context(w) $$是[-5, 5]。默认输入单词只计算1次(numIteration = 1)。

```scala
@Since("1.1.0")
class Word2Vec extends Serializable with Logging {

    /**
    * Computes the vector representation of each word in vocabulary.
    * @param dataset an RDD of sentences,
    *                each sentence is expressed as an iterable collection of words
    * @return a Word2VecModel
    */
    @Since("1.1.0")
    def fit[S <: Iterable[String]](dataset: RDD[S]): Word2VecModel = {
        // 统计输入单词次数，过滤出现少于minCount次的单词。minCount默认值是5。
        learnVocab(dataset)
        // 建立二叉树
        createBinaryTree()

        val sc = dataset.context

        val expTable = sc.broadcast(createExpTable())
        val bcVocab = sc.broadcast(vocab)
        val bcVocabHash = sc.broadcast(vocabHash)
        try {
            doFit(dataset, sc, expTable, bcVocab, bcVocabHash)
        } finally {
            expTable.destroy(blocking = false)
            bcVocab.destroy(blocking = false)
            bcVocabHash.destroy(blocking = false)
        }
    }

    private def doFit[S <: Iterable[String]](
    dataset: RDD[S], sc: SparkContext,
    expTable: Broadcast[Array[Float]],
    bcVocab: Broadcast[Array[VocabWord]],
    bcVocabHash: Broadcast[mutable.HashMap[String, Int]]) = {
        // syn0对应词向量v(w)，初始化随机生成一个向量值
        // syn1对应隐藏参数\theta{j-1}^u
        val syn0Global =
            Array.fill[Float](vocabSize * vectorSize)((initRandom.nextFloat() - 0.5f) / vectorSize)
        val syn1Global = new Array[Float](vocabSize * vectorSize)
        ......
            while (pos < sentence.length) {
              val word = sentence(pos)
              val b = random.nextInt(window)
              // Train Skip-gram
              var a = b
              while (a < window * 2 + 1 - b) {
                if (a != window) {
                  val c = pos - window + a
                  if (c >= 0 && c < sentence.length) {
                    val lastWord = sentence(c)
                    val l1 = lastWord * vectorSize
                    val neu1e = new Array[Float](vectorSize)
                    // Hierarchical softmax
                    var d = 0
                    // 对应FOR j = 2:l^u DO
                    while (d < bcVocab.value(word).codeLen) {
                      val inner = bcVocab.value(word).point(d)
                      val l2 = inner * vectorSize
                      // Propagate hidden -> output

                      /** blas sdot */
                      //sdot(int n, float[] sx, int _sx_offset, int incx, float[] sy, int _sy_offset, int incy);
                      // sx = (x0, x1, x2, x3, x4)
                      // sy = (y0, y1, y2, y3, y4)
                      // sdot(2, sx, 1, 2, sy, 3, 1)，表示从索引1开始以步长2从sx取2个数字；从索引3开始以步长1从sy取2个数字，计算x1 * y3 + x3 * y4

                      /** blas saxpy */
                      //saxpy(int n, float sa, float[] sx, int _sx_offset, int incx, float[] sy, int _sy_offset, int incy);
                      // sx = (x0, x1, x2, x3, x4)
                      // sy = (y0, y1, y2, y3, y4)
                      // sdot(2, sa, sx, 1, 2, sy, 3, 1)，表示从索引1开始以步长2从sx取2个数字；从索引3开始以步长1从sy取2个数字，y3 = sa * x1 + y3, y4 = sa * x3 * y4

                      // 对应v(w)^T\theta^{j-1}^u
                      var f = blas.sdot(vectorSize, syn0, l1, 1, syn1, l2, 1)
                      if (f > -MAX_EXP && f < MAX_EXP) {
                        // 不知道为什么q的计算不是用的sigmod函数，而是通过一个expTable来计算
                        // expTable默认是一个长度1000的数组，第500个值等于0.5，两头分别接近0和1
                        val ind = ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2.0)).toInt
                        f = expTable.value(ind)
                        // 对应g = \eta(1 - d_j^u - q)
                        val g = ((1 - bcVocab.value(word).code(d) - f) * alpha).toFloat
                        // 更新e := e + g\theta{j-1}^u
                        blas.saxpy(vectorSize, g, syn1, l2, 1, neu1e, 0, 1)
                        // 更新\theta{j-1}^u := \theta{j-1}^u + gv(w)
                        blas.saxpy(vectorSize, g, syn0, l1, 1, syn1, l2, 1)
                        syn1Modify(inner) += 1
                      }
                      d += 1
                    }
                    blas.saxpy(vectorSize, 1.0f, neu1e, 0, 1, syn0, l1, 1)
                    syn0Modify(lastWord) += 1
                  }
                }
                a += 1
              }
              pos += 1
            }
    }
}
```

参考：  
[Word2Vec的数学原理](https://blog.csdn.net/itplus/article/details/37969519)  
[自己动手写Word2Vec](https://blog.csdn.net/u014595019/article/details/51884529)  
[BLAS方法简介](http://3iter.com/2015/10/22/BLAS%E6%96%B9%E6%B3%95%E7%AE%80%E4%BB%8B%EF%BC%88%E4%B8%80%EF%BC%89/)