---
title:  “Spark ML - 高斯混合模型 & EM算法”
mathjax: true
layout: post
date:   2018-07-10 08:00:12 +0800
categories: machine learning
---

目标：拟合两个高斯分布  
数据源：没有标签，因此并不知道某个数据点是由哪个高斯分布生成的

# **数学推导**

假设一个隐藏随机变量z服从伯努利分布

$$
z \sim \mathcal Bernoulli(\phi) ( \phi_j\geq0, \sum\limits_{j}\phi_j=1)
$$


在z条件下x服从高斯分布

$$
x | z \sim \mathcal N(\mu_j, \Sigma_j)
$$

x, z的联合分布概率为

$$
P(x,z)=P(x|z)P(z) 
$$

假设我们知道z，似然估计函数为
$$ \mathcal{L}(\phi,\mu,\Sigma)=\log \prod\limits_{i=1}^{m}P(x^{(i)},z^{(i)};\phi,\mu,\Sigma) $$，类似一个高斯判别分析模型。然而高斯混合模型是一个无监督学习算法，我们不知道z的分布情况。
因此需要猜测一个z值，然后计算参数$$ \phi,\mu,\Sigma $$，迭代修正z，此即最大期望算法（EM Algorithm）。

E-Step计算隐藏$$ z^{(i)}=j $$值，即对于每个训练数据$$ x^{(i)} $$来说，它由第j个高斯分布生成的概率为：

$$
\begin{aligned}
let\ w_j^{(i)}&=P(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma) \\
&=\frac{P(x^{(i)}|z^{(i)}=j)P(z^{(i)}=j)}{\sum\limits_{k}P(x^{(i)}|z^{(i)}=k)P(z^{(i)}=k)}
\end{aligned}
$$

M-Step的目标是$$ \arg\max\limits_{\phi,\mu,\Sigma} \sum\limits_{i}\sum\limits_{j} w_j^{(i)} \log 
\frac{\frac{1}{(2\pi)^{n/2}\left | \Sigma_j \right|^{1/2}}\exp\left ( -\frac{1}{2}(x^{(i)}-\mu_j)^T\Sigma^{-1}(x^{(i)}-\mu_j) \right ) \cdot \phi_j}{w_j^{(i)}} $$，求偏导可得：

$$
\begin{aligned}
&\phi_j=\frac{1}{m}\sum\limits_{i=1}^{m}w_j^{(i)} \\
&\mu_j=\frac{\sum\limits_{i=1}^{m}w_j^{(i)}x^{(i)}}{\sum\limits_{i=1}^{m}w_j^{(i)}} \\
&\Sigma_j=\frac{\sum\limits_{i=1}^{m}w_j^{(i)}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T}{\sum\limits_{i=1}^{m}w_j^{(i)}}
\end{aligned}
$$

# **Spark ML 2.3.1**

Spark ML 高斯混合模型（GaussianMixture）默认分为2类，最大迭代次数100，收敛容差(Converge Tolerance)0.01。方法run实现EM算法训练模型。这里使用"two cluster"测试用例跟步分析。

```scala
  test("two clusters") {
    val data = sc.parallelize(GaussianTestData.data)

    // we set an initial gaussian to induce expected results
    val initialGmm = new GaussianMixtureModel(
      Array(0.5, 0.5),
      Array(
        new MultivariateGaussian(Vectors.dense(-1.0), Matrices.dense(1, 1, Array(1.0))),
        new MultivariateGaussian(Vectors.dense(1.0), Matrices.dense(1, 1, Array(1.0)))
      )
    )

    val Ew = Array(1.0 / 3.0, 2.0 / 3.0)
    val Emu = Array(Vectors.dense(-4.3673), Vectors.dense(5.1604))
    val Esigma = Array(Matrices.dense(1, 1, Array(1.1098)), Matrices.dense(1, 1, Array(0.86644)))

    val gmm = new GaussianMixture()
      .setK(2)
      .setInitialModel(initialGmm)
      .run(data)

    assert(gmm.weights(0) ~== Ew(0) absTol 1E-3)
    assert(gmm.weights(1) ~== Ew(1) absTol 1E-3)
    assert(gmm.gaussians(0).mu ~== Emu(0) absTol 1E-3)
    assert(gmm.gaussians(1).mu ~== Emu(1) absTol 1E-3)
    assert(gmm.gaussians(0).sigma ~== Esigma(0) absTol 1E-3)
    assert(gmm.gaussians(1).sigma ~== Esigma(1) absTol 1E-3)
  }
```

输入参数data是一个RDD[Vector]:

> data => [-5.1971], [-2.5359], [-3.822], [-5.2211], [-5.0602], [4.7118],[6.8989], [3.4592], [4.6322], [5.7048], [4.6567], [5.5026], [4.5605], [5.2043], [6.2734]

通过asBreeze转为BV[Double] (Breeze Vector，优化向量矩阵运算):

> breezeData => DenseVector(-5.1971), DenseVector(-2.5359), DenseVector(-3.822), DenseVector(-5.2211), DenseVector(-5.0602), DenseVector(4.7118), DenseVector(6.8989), DenseVector(3.4592), DenseVector(4.6322), DenseVector(5.7048), DenseVector(4.6567), DenseVector(5.5026), DenseVector(4.5605), DenseVector(5.2043), DenseVector(6.2734)

```scala
  /**
   * Perform expectation maximization
   */
  @Since("1.3.0")
  def run(data: RDD[Vector]): GaussianMixtureModel = {
    val sc = data.sparkContext

    // we will operate on the data as breeze data
    val breezeData = data.map(_.asBreeze).cache()
```

如果输入特征维度超过25，需要进行分布式并行计算（除非分类k特别小）。例子中输入特征向量长度为1。

```scala
    // Get length of the input vectors
    val d = breezeData.first().length
    require(d < GaussianMixture.MAX_NUM_FEATURES, s"GaussianMixture cannot handle more " +
      s"than ${GaussianMixture.MAX_NUM_FEATURES} features because the size of the covariance" +
      s" matrix is quadratic in the number of features.")

    val shouldDistributeGaussians = GaussianMixture.shouldDistributeGaussians(k, d)
```

然后开始初始化混合高斯模型。如果用户没有提供初始化模型，对breezeData数据抽样(k * nSample，默认nSampe=5），初始化一个多元高斯分布。例子中每个高斯分布权重为1/k，weights = [0.5, 0.5]。gaussians数组中每个MultivariateGaussian的均值$$ \mu $$为$$ \mathbb{R}^{1} $$向量，方差 $$ \Sigma $$为$$ \mathbb{R}^{1 \times 1} $$矩阵。

> samples => DenseVector(-2.5359), DenseVector(4.6567), DenseVector(5.5026), DenseVector(4.7118), DenseVector(4.6322), DenseVector(-5.2211), DenseVector(5.5026), DenseVector(-5.2211), DenseVector(3.4592), DenseVector(4.7118)

```scala
    // Determine initial weights and corresponding Gaussians.
    // If the user supplied an initial GMM, we use those values, otherwise
    // we start with uniform weights, a random mean from the data, and
    // diagonal covariance matrices using component variances
    // derived from the samples
    val (weights, gaussians) = initialModel match {
      case Some(gmm) => (gmm.weights, gmm.gaussians)

      case None =>
        val samples = breezeData.takeSample(withReplacement = true, k * nSamples, seed)
        (Array.fill(k)(1.0 / k), Array.tabulate(k) { i =>
          val slice = samples.view(i * nSamples, (i + 1) * nSamples)
          new MultivariateGaussian(vectorMean(slice), initCovariance(slice))
        })
    }
```

下面是核心的EM算法迭代部分。ExpectationSum.add方法定义了treeAggregate算子的seqOp（map），用于计算一个训练样本点的分布概率和对数似然估计；ExpectationSum +=定义了treeAggregate算子的combOp（reduce），用于合并分区结果。breezeData.treeAggregate即EM算法中的Expectation步骤。updateWeightsAndGaussians用于更新均值，协方差和权重参数，即EM算法中Maximization步骤。

```scala
    var llh = Double.MinValue // current log-likelihood
    var llhp = 0.0            // previous log-likelihood

    var iter = 0
    while (iter < maxIterations && math.abs(llh-llhp) > convergenceTol) {
      // create and broadcast curried cluster contribution function
      val compute = sc.broadcast(ExpectationSum.add(weights, gaussians)_)

      // aggregate the cluster contribution for all sample points
      val sums = breezeData.treeAggregate(ExpectationSum.zero(k, d))(compute.value, _ += _)

      // Create new distributions based on the partial assignments
      // (often referred to as the "M" step in literature)
      val sumWeights = sums.weights.sum

      if (shouldDistributeGaussians) {
        val numPartitions = math.min(k, 1024)
        val tuples =
          Seq.tabulate(k)(i => (sums.means(i), sums.sigmas(i), sums.weights(i)))
        val (ws, gs) = sc.parallelize(tuples, numPartitions).map { case (mean, sigma, weight) =>
          updateWeightsAndGaussians(mean, sigma, weight, sumWeights)
        }.collect().unzip
        Array.copy(ws, 0, weights, 0, ws.length)
        Array.copy(gs, 0, gaussians, 0, gs.length)
      } else {
        var i = 0
        while (i < k) {
          val (weight, gaussian) =
            updateWeightsAndGaussians(sums.means(i), sums.sigmas(i), sums.weights(i), sumWeights)
          weights(i) = weight
          gaussians(i) = gaussian
          i = i + 1
        }
      }

      llhp = llh // current becomes previous
      llh = sums.logLikelihood // this is the freshly computed log-likelihood
      iter += 1
      compute.destroy(blocking = false)
    }

    new GaussianMixtureModel(weights, gaussians)
  }
```

混合高斯模型预测方法predict调用computeSoftAssignments方法，计算需要预测的样本点，属于各个分布的概率，然后选取其中最大值。
```scala
  /**
   * Maps given point to its cluster index.
   */
  @Since("1.5.0")
  def predict(point: Vector): Int = {
    val r = predictSoft(point)
    r.indexOf(r.max)
  }

  /**
   * Given the input vector, return the membership values to all mixture components.
   */
  @Since("1.4.0")
  def predictSoft(point: Vector): Array[Double] = {
    computeSoftAssignments(point.asBreeze.toDenseVector, gaussians, weights, k)
  }

  /**
   * Compute the partial assignments for each vector
   */
  private def computeSoftAssignments(
      pt: BreezeVector[Double],
      dists: Array[MultivariateGaussian],
      weights: Array[Double],
      k: Int): Array[Double] = {
    val p = weights.zip(dists).map {
      case (weight, dist) => MLUtils.EPSILON + weight * dist.pdf(pt)
    }
    val pSum = p.sum
    for (i <- 0 until k) {
      p(i) /= pSum
    }
    p
  }
```

参考： 
[知乎：多维高斯分布](https://www.zhihu.com/question/36339816)