---
title:  “Object Detection”
date:   2019-08-20 08:00:12 +0800
categories: deeplearning.ai
---

目标检测(Object Detection)是卷积神经网络在计算机视觉领域的一个重要应用，其基础是目标定位(Object Localization)。用汽车的图片举例，目标定位不仅要用算法判断图片中是不是一辆汽车，还要在标记出汽车在图片中的位置；目标检测则需要在图片检测出多个对象并确定位置。


# 分类定位问题

在之前的课程中，已经学习了图片分类问题，即输入一张图片到多层卷积神经网络，输出一个特征向量给Softmax单元来预测图片类型。如果还想定位图片中汽车的位置，可以让神经网络多输出一个边界框$$b_x, b_y, b_h, b_w$$。具体的目标标签$$y$$定义如下：

$$
y = \left [
        \begin{matrix}
            p_c \\ b_x \\ b_y \\ b_h \\ b_w \\
            c_1 \\ c_2 \\ c_3       
        \end{matrix}
    \right ]
$$

$$p_c$$表示是否含有对象，$$c_1,c_2,c_3$$表示该对象属于行人，汽车，摩托车中的哪一类。神经网络的损失函数如果采用平方误差策略，则$$L(\hat y,y)=(\hat y_1 - y_1)^2 + (\hat y_2 - y_2)^2 + \cdots + (\hat y_8 - y_8)^2$$。当图片中存在定位对象，即$$p_c=1$$，损失值就是不同元素的平方和；当图片中不存在定位对象，也就是$$p_c=0$$，不用考虑其他元素，损失值是$$(\hat y_1 - y_1)^2$$。在实际应用中，也可以对边界框应用平方差，对$$p_c$$应用逻辑回归函数。

# 目标检测

目标检测最基本的一个想法就是基于滑动窗口的检测。首先通过一个标签训练集，训练一个卷积网络判断图片有汽车或者没有汽车。假设测试一张图片:
1. 选定一个特定大小的窗口，将红色小方块输入卷积神经网络，开始预测红色方框内有没有汽车
2. 将红色方框向右滑动后的区域输入给卷积网络，依次重复，直到窗口滑过图像的每一个角落
3. 选择一个更大的窗口，重复上述操作。

![image01]({{site.baseurl}}/image/20190820/slide_window.png)

**问题：上述基于滑动窗口的对象检测算法，效率很低。**

## 滑动窗口的卷积实现

为了构建滑动窗口的卷积应用，首先把神经网络的全连接层转化成卷积层。大小为$$5 \times 5 \times 16$$的输入图像，用400个$$5 \times 5 \times 5 \times 16$$的过滤器对它进行卷积，输出维度为$$1 \times 1 \times 400$$。从数学角度看，它和全连接层是一样的。

![image02]({{site.baseurl}}/image/20190820/fc2conv.png)

滑动窗口卷积操作的原理是不需要把输入图像分割成多个子集，分别执行前向传播，而是把它们作为一张图片输入给卷积网络进行计算，其中的公共区域可以共享部分计算结果。下面用一张$$28 \times 28 \times 3$$的图片为例，以$$14 \times 14$$区域滑动窗口，最终输出$$8 \times 8 \times 4$$的结果(注：图片中$$16 \times 16$$应为$$24 \times 24$$)。

![image03]({{site.baseurl}}/image/20190820/slide_conv.png)

**问题：滑动窗口算法的边界框位置可能不够准确。**

## YOLO算法

一个能得到更精准边界框的算法是YOLO算法，意思是You Only Look Once。YOLO基本思路就是前面介绍的图像分类定位算法，但是要在图像上放一个网格。网格大小根据图片大小自己选择，例子中选用$$3 \times 3$$，实际应用中可以用更精细的网格$$19 \times 19$$。然后将图像中的对象分配给包含对象中点的格子，即使对象横跨多个格子，也只会被分配到其中一个格子。在汽车检测的例子里，神经网络的训练输入为$$100 \times 100 \times 3$$的图像，然后接一个普通的卷积网络（卷积层，最大池化层等等），最后映射到一个$$3 \times 3 \times 8$$的输出。

在YOLO算法中，约定格子左上点坐标是$$(0,0)$$，右下点坐标是$$(1,1)$$。边界框的中心坐标$$b_x$$和$$b_y$$的取值在0和1直接，高度$$b_h$$和宽度$$b_w$$用格子长宽的比例表示，这个值可能会大于1。YOLO算法是一个单次卷积实现，所以不需要循环，运行速度非常快。

![image04]({{site.baseurl}}/image/20190820/yolo.png)

**问题：对同一对象作出多次检测**

## 非极大值抑制（Non-max Suppression)

首先需要介绍一下“交并比”函数。交并比(IoU)计算两个边界框交集和并集区域之比(下图中的橙色阴影和绿色阴影)，用于评价对象检测算法。一般约定，0.5是阈值，判断预测的编辑框是否正确。如果希望严格一点，可以把阈值定为0.6甚至0.7。

![image05]({{site.baseurl}}/image/20190820/iou.png)

如果在$$361(19 \times 19)$$个格子上运行一次图像分类和定位算法，那么可能很多格子的$$p_c$$都会 超过阈值，而不仅仅只有两个格子报告检测出一个对象。非极大值抑制做的就是清理这些检测结果。

具体而言，首先看看每个格子检测结果相关的概率$$p_c$$，找到概率最大的那个，这个例子中右边的车辆是0.9。我们认为这是当前最可靠的检查，所以高亮标记。逐一审视剩下的矩形，所有和这个最大边框有很高交并比，高度重叠的其他边界框都会被抑制。在这个例子中，被抑制边界框的$$p_c$$分别是0.6和0.7。

接下来，再次找到概率$$p_c$$最高的一个边界框，是左边车辆的0.8。然后非极大值抑制算法去掉其他IoU值很高的矩形，最后得到两个预测结果。

![image06]({{site.baseurl}}/image/20190820/non_max_suppress.png)

**问题：一个格子中存在多个对象**

## Anchor Boxes

为了让一个格子检测出多个对象，需要预先定义不同性质的anchor box，把预测结果和anchor box关联起来。比如行人的边界框更类似于anchor box 1的形状，而汽车的边界框更类似于anchor box 2的形状。$$y$$的定义不再是$$[p_c \quad b_x \quad b_y \quad b_h \quad b_w \quad c_1 \quad c_2 \quad c_3]^T$$，而是$$[p_c \quad b_x \quad b_y \quad b_h \quad b_w \quad c_1 \quad c_2 \quad c_3 \quad p_c \quad b_x \quad b_y \quad b_h \quad b_w \quad c_1 \quad c_2 \quad c_3]^T$$。

![image07]({{site.baseurl}}/image/20190820/anchor_box.png)

在引入anchor box后，对象不只分配到一个格子里，而是分配到一对$$(grid\ cell，anchor\ box)$$里，其中anchor box是和实际边界框交并比比较高的那一个。现在的输出$$y$$就是$$3 \times 3 \times 16$$或者$$3 \times 3 \times 2 \times 8$$。

观察下图中格子编号2，行人更类似于anchor box 1的形状，所以对于行人来说，将分配到向量的上半部分。这里存在一个对象，即$$p_c=1$$，如果行人类别是1，那么$$c_1=1, c_2=0, c_3=0$$（编号1所示橙色部分参数）。车子形状更像anchor box 2的形状，所以向量剩下部分$$p_c=1, c_1=0, c_2=1, c_3=0$$（编号1所示绿色部分参数）。

![image08]({{site.baseurl}}/image/20190820/anchor_box_example.png)

如果有两个anchor box，但同一个格子中有三个对象，上述算法处理不好。另外，人们一般手工指定anchor box形状，可以选择5到10个覆盖多种不同的形状。