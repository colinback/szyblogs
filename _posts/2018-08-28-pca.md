---
title:  “Spark ML - PCA”
date:   2018-08-28 08:00:12 +0800
categories: machine learning
---

主成分析（PCA）是一种降维的方法，通过将高维数据投影到低维空间，消除冗余数据。主成分析法的目标是找到一个超平面，使得高维数据投影到这个超平面后尽量分散。


1. 首先数据归一化

    $$
    \begin{aligned}
    &\mu = \frac{1}{m} \sum_{i=1}^m x^{(i)} \\
    &x^{(i)} = x^{(i)} - \mu \\
    &\sigma_j^2 = \frac{1}{m} \sum_{i=1}^m (x_j^{(i)})^2 \\
    &x_j^{(i)} = x_j^{(i)} / \sigma_j
    \end{aligned}
    $$

2. PCA的理论之一是最大方差理论，要求找到最佳向量$$u=(u_1, u_2, \dots, u_k)^T$$，使得投影(点乘$$x^{(i)} \centerdot u$$)后的样本点方差最大。

    $$
    \begin{aligned}
    \lambda &= \frac{1}{m} \sum_{i=1}^m (u^T x^{(i)})^2 \\
    &= \frac{1}{m} \sum_{i=1}^m u^T x^{(i)} {x^{(i)}}^T u \\
    &= u^T \left ( \frac{1}{m} \sum_{i=1}^m x^{(i)} {x^{(i)}}^T \right ) u \\
    &= u^T \Sigma u
    \end{aligned}
    $$

    $$\Sigma$$即是样本的协方差矩阵，$$\lambda$$是$$\Sigma$$的特征值，$$u$$是特征向量。最佳投影即是特征值$$\lambda$$最大时对应的特征向量，以此类推。选取前k个特征向量投影得到新的第i个样本$$(u_1^Tx^{(i)}, u_2^Tx^{(i)}, \dots, u_k^Tx^{(i)})^T$$

PCA有很多的应用场景：
- 压缩数据
- 可视化
- 降维防止过拟合
- 异常检测，通过PCA可以找到由k个主成分组成的超平面，如果新的数据离该超平面很远，就说明可能是异常数据

理论上只需对协方差矩阵进行特征值分解，得到前k个最大特征值对应特征向量，就是最佳的k维新特征。但是一个现实问题是，如果特征$$x$$维度很高，则协方差矩阵的特征值计算非常困难。此时可以应用SVD分解，假设m个样本，每个样本包含n个特征，用矩阵$$A \in \mathbb{R}^{m \times n}$$表示。

$$
A_{m \times n} = U_{m \times m} D_{m \times n} V^T_{n \times n}
$$

矩阵$$D$$除了对角线的元素都是0，对角线上元素称为奇异值。我们可以用前r个最大的奇异值来近似描述矩阵，矩阵$$V$$的r个列即是协方差矩阵$$\Sigma=A^TA \in \mathbb{R}^{n \times n}$$最大的r个特征向量。

$$
A_{m \times n} \approx U_{m \times r} D_{r \times r} V^T_{r \times n}
$$

机器学习中SVD另一个常见的应用是协同过滤(Collaborative Filtering)。

参考：  
[cnblog - 主成分析最大方差解释](http://www.cnblogs.com/jerrylead/archive/2011/04/18/2020209.html)  
[cnblog - 奇异值分解SVD及其应用](http://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html)